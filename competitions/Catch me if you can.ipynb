{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a template for your reproducible solution in the Alice competition.\n",
    "It's obligatory that your script produces a submission file just \n",
    "by running `python solution_alice_<name>_<surname>.py`. \n",
    "If you have any dependecies apart from those in a Kaggle Docker image, \n",
    "it's your responsibility to provide an image (or at least a requirements file) \n",
    "to reproduce your solution.\n",
    "\n",
    "Please avoid heavy hyperparameter optimization in this script. \n",
    "\n",
    "IMPORTANT: this script is to be shared only with organizers, as described in the\n",
    "course roadmap https://mlcourse.ai/roadmap. Be careful not to share it in \n",
    "Kaggle Kernels, don't spoil the competitive spirit. \n",
    "'''\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "PATH_TO_DATA = '../input'\n",
    "AUTHOR = 'Vladimir_Kulyashov' # change here to <name>_<surname>\n",
    "# it's a nice practice to define most of hyperparams here\n",
    "SEED = 17\n",
    "N_JOBS = 4\n",
    "NUM_TIME_SPLITS = 10    # for time-based cross-validation\n",
    "SITE_NGRAMS = (1, 5)    # site ngrams for \"bag of sites\"\n",
    "MAX_FEATURES = 50000    # max features for \"bag of sites\"\n",
    "BEST_LOGIT_C = 5.45559  # precomputed tuned C for logistic regression\n",
    " \n",
    "\n",
    "# nice way to report running times\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "def prepare_sparse_features(path_to_train, path_to_test, path_to_site_dict,\n",
    "                           vectorizer_params):\n",
    "    times = ['time%s' % i for i in range(1, 11)]\n",
    "    train_df = pd.read_csv(path_to_train,\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "    test_df = pd.read_csv(path_to_test,\n",
    "                      index_col='session_id', parse_dates=times)\n",
    "\n",
    "    # Sort the data by time\n",
    "    train_df = train_df.sort_values(by='time1')\n",
    "    \n",
    "    # read site -> id mapping provided by competition organizers \n",
    "    with open(path_to_site_dict, 'rb') as f:\n",
    "        site2id = pickle.load(f)\n",
    "    # create an inverse id _> site mapping\n",
    "    id2site = {v:k for (k, v) in site2id.items()}\n",
    "    # we treat site with id 0 as \"unknown\"\n",
    "    id2site[0] = 'unknown'\n",
    "    \n",
    "    # Transform data into format which can be fed into TfidfVectorizer\n",
    "    # This time we prefer to represent sessions with site names, not site ids. \n",
    "    # It's less efficient but thus it'll be more convenient to interpret model weights.\n",
    "    sites = ['site%s' % i for i in range(1, 11)]\n",
    "    train_sessions = train_df[sites].fillna(0).astype('int').apply(lambda row: \n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    test_sessions = test_df[sites].fillna(0).astype('int').apply(lambda row: \n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    # we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n",
    "    # so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n",
    "    # to be split into 'mail', 'google' and 'com')\n",
    "    vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "    X_train = vectorizer.fit_transform(train_sessions)\n",
    "    X_test = vectorizer.transform(test_sessions)\n",
    "    y_train = train_df['target'].astype('int').values\n",
    "    \n",
    "    # we'll need site visit times for further feature engineering\n",
    "    train_times, test_times = train_df[times], test_df[times]\n",
    "    \n",
    "    return X_train, X_test, y_train, vectorizer, train_times, test_times\n",
    "\n",
    "\n",
    "def add_features(times, X_sparse):\n",
    "    hour = times['time1'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int').values.reshape(-1, 1)\n",
    "    sess_duration = (times.max(axis=1) - times.min(axis=1)).astype('timedelta64[s]')\\\n",
    "\t\t   .astype('int').values.reshape(-1, 1)\n",
    "    day_of_week = times['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n",
    "    month = times['time1'].apply(lambda t: t.month).values.reshape(-1, 1) \n",
    "    year_month = times['time1'].apply(lambda t: 100 * t.year + t.month).values.reshape(-1, 1) / 1e5\n",
    "\n",
    "    X = hstack([X_sparse, morning, day, evening, night, sess_duration, day_of_week, month, year_month])\n",
    "    return X\n",
    "\n",
    "\n",
    "with timer('Building sparse site features'):\n",
    "    X_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times = \\\n",
    "        prepare_sparse_features(\n",
    "            path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "            path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "            path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n",
    "            vectorizer_params={'ngram_range': SITE_NGRAMS,\n",
    "                               'max_features': MAX_FEATURES,\n",
    "                               'tokenizer': lambda s: s.split()})\n",
    "\n",
    "\n",
    "with timer('Building additional features'):\n",
    "    X_train_final = add_features(train_times, X_train_sites)\n",
    "    X_test_final = add_features(test_times, X_test_sites)\n",
    "\n",
    "\n",
    "with timer('Cross-validation'):\n",
    "    time_split = TimeSeriesSplit(n_splits=NUM_TIME_SPLITS)\n",
    "    logit = LogisticRegression(random_state=SEED, solver='liblinear')\n",
    "\n",
    "    # I've done cross-validation locally, and do not reproduce these heavy computations here,\n",
    "    # but this is the vest C that I've found\n",
    "    c_values = [BEST_LOGIT_C]\n",
    "\n",
    "    logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=N_JOBS, cv=time_split, verbose=1)\n",
    "    logit_grid_searcher.fit(X_train_final, y_train)\n",
    "    print('CV score', logit_grid_searcher.best_score_)\n",
    "\n",
    "\n",
    "with timer('Test prediction and submission'):\n",
    "    test_pred = logit_grid_searcher.predict_proba(X_test_final)[:, 1]\n",
    "    pred_df = pd.DataFrame(test_pred, index=np.arange(1, test_pred.shape[0] + 1),\n",
    "                       columns=['target'])\n",
    "    pred_df.to_csv(f'submission_alice_{AUTHOR}.csv', index_label='session_id')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
