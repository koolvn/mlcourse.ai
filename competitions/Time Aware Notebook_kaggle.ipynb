{
 "cells": [
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": false
   },
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\" />\n",
    "</center> \n",
    "     \n",
    "---------------------------\n",
    "## <center> Kaggle inclass competition from [mlcourse.ai](https://mlcourse.ai/)\n",
    "    \n",
    "# <center> [**Catch me if you can**](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2)\n",
    "\n",
    "### <center> Session: Fall 2019\n",
    "\n",
    "#### <div style=\"text-align: right\"> Author: [Vladimir Kulyashov](https://github.com/koolvn)\n",
    "\n",
    "<div style=\"text-align: right\"> creation date: 15 October 2019 </div>\n",
    "\n",
    "-------------\n",
    "\n",
    "**Prerequisitions proved by EDA:**\n",
    "1. Alice lives in France\n",
    "2. Data was collected in the university during working hours \n",
    "3. Alice surfed the web mostly for watching videos and social networks\n",
    "4. Alice doesn't use GMail or Google+ and Bing\n",
    "5. We also know how Alice behave herself in the internet during the year, month, week, day\n",
    "\n",
    "\n",
    "**Goal:**\n",
    "1. Beat the A3 strong baseline (0.95965) baseline with as less features as possible\n",
    "2. Improve as far as possible\n",
    "\n",
    "-------"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Import libraries and set desired options\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "# !pip install eli5\n",
    "import eli5\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display_html"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "PATH_TO_DATA = '/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/'\n",
    "filename = f'submission_1.csv'\n",
    "pred_path = './'\n",
    "SEED = 17\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "logit = LogisticRegression(C=1, random_state=SEED, solver='liblinear')\n",
    "BEST_LOGIT_C = 5.0118 # 2.5118864315095824 # 2.9286445646252366\n",
    "BEST_LOGIT_TOL = 0.045 # 0.089"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def prepare_sparse_features(path_to_train, path_to_test, path_to_site_dict,\n                           vectorizer_params):\n    \"\"\" Prepares sparsed X_train, X_test, y_train, vectorizer, train_times, test_times,\n        train_sites, test_sites, top_alice_sites\n        \n        from input CSV files, pickle file and vectorizer_params dictionary.\n    \n        return:: X_train, X_test, y_train, vectorizer, train_times, test_times, train_sites, test_sites, top_alice_sites \"\"\"\n    \n    times = ['time%s' % i for i in range(1, 11)]\n    train_df = pd.read_csv(path_to_train,\n                       index_col='session_id', parse_dates=times)\n    test_df = pd.read_csv(path_to_test,\n                      index_col='session_id', parse_dates=times)\n\n    # Sort the data by time\n    train_df = train_df.sort_values(by='time1')\n    \n    # read site -> id mapping provided by competition organizers \n    with open(path_to_site_dict, 'rb') as f:\n        site2id = pickle.load(f)\n    # create an inverse id _> site mapping\n    id2site = {v:k for (k, v) in site2id.items()}\n    # we treat site with id 0 as \"unknown\"\n    id2site[0] = 'unknown'\n    \n    # Transform data into format which can be fed into TfidfVectorizer\n    # This time we prefer to represent sessions with site names, not site ids. \n    # It's less efficient but thus it'll be more convenient to interpret model weights.\n    sites = ['site%s' % i for i in range(1, 11)]\n    train_sessions = train_df[sites].fillna(0).astype('int').apply(lambda row: \n                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    test_sessions = test_df[sites].fillna(0).astype('int').apply(lambda row: \n                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    \n    sites_dict = pd.DataFrame(list(site2id.keys()),\n                              index=list(site2id.values()),\n                              columns=['site'])\n    \n    top_alice_sites = pd.Series(train_df[train_df['target'] == 1][sites].fillna(0).astype('int').values.flatten()\n                               ).value_counts().sort_values(ascending=False).head(5)\n    # we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n    # so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n    # to be split into 'mail', 'google' and 'com')\n    vectorizer = TfidfVectorizer(**vectorizer_params)\n    X_train = vectorizer.fit_transform(train_sessions)\n    X_test = vectorizer.transform(test_sessions)\n    y_train = train_df['target'].astype('int').values\n    \n    # we'll need site visit times for further feature engineering\n    train_times, test_times = train_df[times], test_df[times]\n    \n    # sites_df\n    train_sites, test_sites = train_df[sites].fillna(0).astype('int'), test_df[sites].fillna(0).astype('int')\n    \n    full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n    \n    return X_train, X_test, y_train, vectorizer, train_times, test_times, train_sites, test_sites, top_alice_sites",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "%%time\nX_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times, train_sites, test_sites, top_alice_sites = prepare_sparse_features(\n    path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n    path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n    path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n    vectorizer_params={'ngram_range': (1, 5), \n                       'max_features': 50000,\n                       'tokenizer': lambda s: s.split()}\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## EDA"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of sessions by month"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "start_alice = train_times['time1'][y_train == 1]\nstart_other = train_times['time1'][y_train == 0]\n\nstart_alice_full_month = start_alice.apply(lambda ts: 100 * ts.year + ts.month).astype('int')\nstart_other_full_month = start_other.apply(lambda ts: 100 * ts.year + ts.month).astype('int')\n\nplt.subplots(1, 2, figsize = (17, 12))\nplt.rcParams['figure.facecolor'] = 'white'\n\nplt.subplot(221)\nsns.countplot(start_alice_full_month)\nplt.xticks(rotation=30)\nplt.title('Alice sessions by month')\nplt.xlabel('Year + month')\nplt.ylabel('Count')\n\nplt.subplot(222)\nsns.countplot(start_other_full_month)\nplt.xticks(rotation=30)\nplt.title('Others sessions by month')\nplt.xlabel('Year + month')\nplt.ylabel('Count')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Destribution of activity during the week"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "start_week_alice = train_times['time1'][y_train == 1].dt.weekday\nstart_week_other = train_times['time1'][y_train == 0].dt.weekday\n\nplt.subplots(1, 2, figsize = (17, 12))\n\nplt.subplot(221)\nsns.countplot(start_week_alice)\nplt.title('Alice activity during a week')\nplt.xlabel('Day of a week')\nplt.ylabel('Count')\n\nplt.subplot(222)\nsns.countplot(start_week_other)\nplt.title('Others activity during a week')\nplt.xlabel('Day of a week')\nplt.ylabel('Count')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "plt.subplots(1, 2, figsize = (17, 12))\n\nplt.subplot(221)\nsns.countplot(start_alice.dt.hour)\nplt.title('Alice session start hour')\nplt.xlabel('Hours')\nplt.ylabel('Count')\n\nplt.subplot(222)\nsns.countplot(start_other.dt.hour)\nplt.title('Others session start hour')\nplt.xlabel('Hours')\nplt.ylabel('Count')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Activity distribution by hours"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Distribution of test data by months, days and hours"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "plt.subplots(2, 2, figsize = (17, 12))\n\nplt.subplot(221)\nsns.countplot(test_times['time1'].dt.month)\nplt.title('Distribution by months')\nplt.xlabel('Month')\nplt.ylabel('Count')\n\nplt.subplot(222)\nsns.countplot(test_times['time1'].dt.weekday)\nplt.title('Distribution by week days')\nplt.xlabel('Day')\nplt.ylabel('Count')\n\nplt.subplot(223)\nsns.countplot(test_times['time1'].dt.hour)\nplt.title('Distribution by hours')\nplt.xlabel('Hour')\nplt.ylabel('Count')\n\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now it's time to add some features and train first models"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Some helpfull functions\n\n# A helper function for writing predictions to a file and write list of features of that predictions\ndef write_to_submission_file(predicted_labels, out_file, new_feature_names=None, best_params=None, best_score=None,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)\n    \n    text_file = open(out_file+'.txt', \"w\")\n    text_file.write(f'Features:\\n{str(new_feature_names)}\\nParams: {best_params}\\nBest Score: {best_score}')\n    text_file.close()\n\n# I'm lazy, so here is a function that names files for you    \ndef get_next_filename(filename='submission_1.csv', path=pred_path, file_exists=False, i=1):\n    if filename in os.listdir(path):\n        file_exists = True\n        while file_exists:\n            i += 1\n            next_ = list(filename.split('.')[0].split('_')[0])\n            next_.append('_')\n            next_.append(str(i))\n            next_ = ''.join(next_) + '.csv'\n            next_, file_exists = get_next_filename(next_, path, False, i)\n            \n        return next_, file_exists  \n    else:\n        file_exist = False\n        return filename, file_exists\n    \ndef train_and_predict(model, X_train, y_train, X_test, site_feature_names=vectorizer.get_feature_names(), \n                      new_feature_names=None, cv=time_split, scoring='roc_auc',\n                      top_n_features_to_show=30, submission_file_name='submission.csv', best_params=None):\n    \n    \n    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, \n                            scoring=scoring, n_jobs=4)\n    print('CV scores', cv_scores)\n    print('\\nCV mean: {}, CV std: {}'.format(cv_scores.mean(), cv_scores.std()))\n    model.fit(X_train, y_train)\n    \n    if new_feature_names:\n        all_feature_names = site_feature_names + new_feature_names \n    else: \n        all_feature_names = site_feature_names\n    \n    display_html(eli5.show_weights(estimator=model, \n                  feature_names=all_feature_names, top=top_n_features_to_show))\n    \n    if new_feature_names:\n        print('New feature weights:')\n    \n        print(pd.DataFrame({'feature': new_feature_names, \n                        'coef': model.coef_.flatten()[-len(new_feature_names):]}).sort_values(by='coef'))\n    \n    test_pred = model.predict_proba(X_test)[:, 1]\n    write_to_submission_file(test_pred, submission_file_name, new_feature_names, best_params,\n                             best_score=f'\\nCV max: {cv_scores.max()} CV mean: {cv_scores.mean()}, CV std: {cv_scores.std()}') \n    \n    return cv_scores",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Base score with no features added"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "%%time\ncv_scores_base = train_and_predict(model=logit,\n                               X_train=X_train_sites,\n                               y_train=y_train,\n                               X_test=X_test_sites,\n                               site_feature_names=vectorizer.get_feature_names(),\n                               top_n_features_to_show=20,\n                               cv=time_split, submission_file_name=pred_path+'base_'+filename)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Thank's to TfidfVectorizer and eli5 we can see which sites Alice love to visit and which she doesn't use at all"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "markdown",
   "source": "### Adding features"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def add_features(times, sites, X_sparse, top_alice_sites):\n    \n    scaler = StandardScaler()\n#     scaler = MinMaxScaler()\n    \n    with open(PATH_TO_DATA + 'site_dic.pkl', \"rb\") as input_file:\n        site_dict = pickle.load(input_file)\n        \n        \n    sites_dict = pd.DataFrame(list(site_dict.keys()),\n                              index=list(site_dict.values()),\n                              columns=['site'])\n        \n    # time features\n    hour = times['time1'].dt.hour\n    morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n    day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n    evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n    night = ((hour >= 0) & (hour <=6)).astype('int').values.reshape(-1, 1)\n    alice_hours = hour.isin([12,13,16,17,18]).astype('int').values.reshape(-1, 1)\n    alice_days = times['time1'].dt.weekday.isin([0, 1, 3, 4]).astype('int').values.reshape(-1, 1)\n    not_alice_hours = hour.isin([7,8,19,20,21,22,23]).astype('int').values.reshape(-1, 1)\n    \n    durations = (times.max(axis=1) - times.min(axis=1)).astype('timedelta64[ms]').astype('int').values.reshape(-1, 1)\n    durations = scaler.fit_transform(durations)\n    \n    week = times['time1'].dt.week.values.reshape(-1, 1)\n    week = scaler.fit_transform(week)\n\n    winter = times['time1'].dt.month.isin([12, 1, 2]).astype('int').values.reshape(-1, 1)\n    spring = times['time1'].dt.month.isin([3, 4, 5]).astype('int').values.reshape(-1, 1)\n    summer = times['time1'].dt.month.isin([6, 7, 8]).astype('int').values.reshape(-1, 1)\n    autumn = times['time1'].dt.month.isin([9, 10, 11]).astype('int').values.reshape(-1, 1)\n    \n    day_of_week = times['time1'].dt.weekday.astype('int').values.reshape(-1, 1)\n    month = times['time1'].dt.month.astype('int').values.reshape(-1, 1)\n    year_month = times['time1'].apply(lambda ts: 100 * ts.year + ts.month).astype('int').values.reshape(-1, 1)\n    year_month = scaler.fit_transform(year_month)\n    \n    sunday = (times['time1'].dt.weekday == 6).astype('int').values.reshape(-1, 1)\n    monday = (times['time1'].dt.weekday == 0).astype('int').values.reshape(-1, 1)\n    \n    may = (times['time1'].dt.month == 5).astype('int').values.reshape(-1, 1)\n    october = (times['time1'].dt.month == 10).astype('int').values.reshape(-1, 1)\n    \n    # site features\n    facebook_ids = []\n    youtube_ids = []\n    google_ids = []\n    france_ids = []\n    vk_ids = []\n    msft_ids = []\n    bing_ids = []\n    unknown_ids = []\n    search_ids = []\n\n    for key in list(site_dict.keys()):\n        if 'facebook' in key:\n            facebook_ids.append(site_dict[key])\n        if 'youtube' in key  in key or 'video' in key or 'youwatch' in key:\n            youtube_ids.append(site_dict[key])\n        if 'mail.google.com' in key or 'plus.google.com' in key:\n            google_ids.append(site_dict[key])\n        if '.fr' in key or 'fr.' in key:\n            france_ids.append(site_dict[key])\n        if 'vk.com' in key or 'vk.ru' in key:\n            vk_ids.append(site_dict[key])\n        if 'storage.live' in key or '.live.com' in key or 'fr.msn.com' in key:\n            msft_ids.append(site_dict[key])\n        if 'www.bing.com' in key:\n            bing_ids.append(site_dict[key])\n        if 'www.google.fr' in key:\n            search_ids.append(site_dict[key])\n        if 'unknown' in key:\n            unknown_ids.append(site_dict[key])\n\n    top_alice_ids = []\n\n    for key in top_alice_sites.index:\n        top_alice_ids.append(key)\n    \n    first_3 = sites[['site1', 'site2', 'site3']]\n\n    in_alice_top = first_3.isin(top_alice_sites).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    \n    start_google = first_3.isin(google_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    has_google = sites.isin(google_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    first_3_has_google = first_3.isin(google_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    start_youtube = sites['site1'].isin(youtube_ids).astype('int').values.reshape(-1, 1)\n    has_youtube = sites.isin(youtube_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    first_3_has_youtube = first_3.isin(youtube_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    start_facebook = sites['site1'].isin(facebook_ids).astype('int').values.reshape(-1, 1)\n    has_facebook = sites.isin(facebook_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    first_3_has_facebook = first_3.isin(facebook_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    start_vk = sites['site1'].isin(vk_ids).astype('int').values.reshape(-1, 1)\n    has_vk = sites.isin(vk_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    first_3_has_vk = first_3.isin(vk_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    fr_domains = sites.isin(france_ids).astype('int').sum(axis=1).values.reshape(-1, 1)\n    fr_domains = scaler.fit_transform(fr_domains)\n#     fr_domains = sites.isin(france_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    msft_usage = sites.isin(msft_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    has_bing = first_3.isin(bing_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    search = first_3.isin(search_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    unknown = sites.isin(unknown_ids).astype('int').sum(axis=1).astype('bool').astype('int').values.reshape(-1, 1)\n    \n    # stacking matrix\n    objects_to_hstack = [X_sparse,\n                         morning, day, evening, night,\n                         durations,\n                         day_of_week,\n                         year_month,\n                         sunday,\n                         start_google,\n                         start_youtube,\n                         fr_domains,\n                         start_vk,\n                         msft_usage,\n                         summer,\n#                          week,\n                         has_google,\n                         has_youtube,\n                         in_alice_top,\n                         has_vk,\n                         has_facebook,\n                         may,\n                         october,\n#                          has_bing,\n                         first_3_has_vk,\n                         first_3_has_youtube,\n                         first_3_has_facebook,\n#                          search,\n                         alice_hours,\n#                          alice_days,\n#                          first_3_has_google,\n#                          monday,\n#                          unknown,\n                         not_alice_hours,\n                        ]\n    \n    feature_names = ['morning', 'day','evening', 'night',\n                     'durations',\n                     'day_of_week',\n                     'year_month',\n                     'sunday',\n                     'start_google',\n                     'start_youtube',\n                     'fr_domains',\n                     'start_vk',\n                     'msft_usage', \n                     'summer',\n#                      'week',\n                     'has_google',\n                     'has_youtube',\n                     'in_alice_top',\n                     'has_vk',\n                     'has_facebook',\n                     'may',\n                     'october',\n#                      'has_bing',\n                     'first_3_has_vk',\n                     'first_3_has_youtube',\n                     'first_3_has_facebook',\n#                      'search',\n                     'alice_hours',\n#                      'alice_days',\n#                      'first_3_has_google',\n#                      'monday',\n#                      'unknown',\n                     'not_alice_hours',\n                    ]\n        \n    X = hstack(objects_to_hstack)\n    return X, feature_names",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### At this point we are actually \"overfitting\" to Alice behavior...\n### On the other hand we _are_ to do so, because we have to seporate Alice from others."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training model with added features"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "%%time\nX_train, new_feat_names = add_features(train_times, train_sites, X_train_sites, top_alice_sites)\nX_test, _ = add_features(test_times, test_sites, X_test_sites, top_alice_sites)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "new_feat_names",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "%%time\ncv_scores_engineered = train_and_predict(model=logit, X_train=X_train, y_train=y_train,\n                                         X_test=X_test, \n                                         site_feature_names=vectorizer.get_feature_names(),\n                                         new_feature_names=new_feat_names,\n                                         cv=time_split,\n                                         submission_file_name=pred_path + 'engineered_' + filename)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "cv_scores_base < cv_scores_engineered",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we've got 0.96463 on the LB. Let's tune the params and look if we get better results"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tuning hyperparameters"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "%%time\nlogit = LogisticRegression(random_state=SEED, solver='liblinear')\n\nparams = {'C': [BEST_LOGIT_C],\n          'tol': [BEST_LOGIT_TOL]}\n\nlogit_grid_searcher = GridSearchCV(estimator=logit, param_grid=params,\n                              scoring='roc_auc', n_jobs=4, cv=time_split, verbose=1)\n\nlogit_grid_searcher.fit(X_train, y_train)\n\nprint('Tuned score', logit_grid_searcher.best_score_)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "logit_grid_searcher.best_estimator_",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training the best model and writing the output file."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "%%time\ncv_scores_tuned = train_and_predict(model=logit_grid_searcher.best_estimator_, X_train=X_train, y_train=y_train,\n                                         X_test=X_test, \n                                         site_feature_names=vectorizer.get_feature_names(),\n                                         new_feature_names=new_feat_names,\n                                         cv=time_split,\n                                         submission_file_name=pred_path + get_next_filename(filename)[0], best_params=logit_grid_searcher.best_params_)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "cv_scores_engineered < cv_scores_tuned",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "logit_grid_searcher.best_params_",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wow! We've got 0.96600 on LB! That's top 1%."
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}